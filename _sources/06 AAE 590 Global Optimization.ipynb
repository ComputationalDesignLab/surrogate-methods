{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96398c7",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">AAE 590 Surrogate Methods</h1>\n",
    "\n",
    "## Global Optimization\n",
    "\n",
    "This notebook supports material covered in the class for global optimization. We will be using `scipy.optimize` module. Following are the topics covered:\n",
    "\n",
    "1. [Multi-start gradient based optimization](#Multi-start-gradient-based-optimization)\n",
    "2. [Differential evolution](#Differential-evolution)\n",
    "\n",
    "<font color='red'>**Please run the below block of code before you run any other block**</font> - it imports all the packages needed for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2935a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d011dee1",
   "metadata": {},
   "source": [
    "Consider the following fourth-order smooth multimodal function (also know as jones function):\n",
    "\n",
    "$$\n",
    "    f(x_1, x_2) = x_1^4 + x_2^4 - 4x_1^3 - 3x_2^3 + 2x_1^2 + 2x_1x_2\n",
    "$$\n",
    "\n",
    "As we have been doing in previous notebooks, we will plot the function contour first. It is very inmportant to have some insights/understanding of the design space. Below block of code defines a python function which returns the value of jones function at any given $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61da62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jones_function(x):\n",
    "    \"\"\"\n",
    "        Function to evaluate values of jones function at any given x.\n",
    "        \n",
    "        Input:\n",
    "        x - 1d numpy array containing only two entries. First entry is x1\n",
    "        and 2nd entry is x2.\n",
    "    \"\"\"\n",
    "    \n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    \n",
    "    return x1**4 + x2**4 - 4*x1**3 - 3*x2**3 + 2*x1**2 + 2*x1*x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8617853",
   "metadata": {},
   "source": [
    "Below block of code plots the contour of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c0607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining x and y values\n",
    "x = np.linspace(-2,4,50)\n",
    "y = np.linspace(-2,4,50)\n",
    "\n",
    "# Creating a mesh at which values and \n",
    "# gradient will be evaluated and plotted\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Evaluating the function values at meshpoints\n",
    "x1 = X.reshape(-1,)\n",
    "x2 = Y.reshape(-1,)\n",
    "Z = np.zeros_like(x1)\n",
    "\n",
    "# Calculating function value at each grid point\n",
    "for i in range(len(x1)):\n",
    "        Z[i] = jones_function(np.array([x1[i],x2[i]]))\n",
    "\n",
    "Z = Z.reshape(X.shape)\n",
    "        \n",
    "# Denoting at which level to add contour lines\n",
    "levels = np.arange(-13,-5,1)\n",
    "levels = np.concatenate((levels, np.arange(-4, 8, 3)))\n",
    "levels = np.concatenate((levels, np.arange(10, 100, 15)))\n",
    "\n",
    "# Plotting the contours\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "CS = ax.contour(X, Y, Z, levels=levels, colors=\"k\", linestyles=\"solid\")\n",
    "ax.clabel(CS, inline=1, fontsize=8)\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=14)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=14)\n",
    "ax.set_title(\"Jones Function\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d71ca12",
   "metadata": {},
   "source": [
    "**Note**: For the jones function, a logscaled filled contour is needed to better visualize the design space. But there are many negative function values in the region and it is not possibile to take log of negative values. So, only line contours are plotted.\n",
    "\n",
    "Before starting with global optimization, we will do simple unconstrained gradient based optimization with different starting points and note the results. Below block of code defines various required functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fafd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jones_opt_history(x):\n",
    "    \"\"\"\n",
    "        Function which is called after every iteration of optimization.\n",
    "        It stores the value of x1, x2, and function value which is later\n",
    "        for plotting convergence history.\n",
    "        \n",
    "        Input: \n",
    "        x - 1d numpy array which contains current x value\n",
    "        convergence - represents the fractional value of the population convergence\n",
    "        Output: None\n",
    "    \"\"\"\n",
    "    \n",
    "    history[\"x1\"].append(x[0])\n",
    "    history[\"x2\"].append(x[1])\n",
    "    history[\"f\"].append(jones_function(x))\n",
    "    \n",
    "def jones_func_opt_plots(history, starting_point):\n",
    "    \"\"\"\n",
    "        Function used for plotting the results of the optimization.\n",
    "        \n",
    "        Input: \n",
    "        history - A dictionary which contains three key-value pairs - x1, x2, and f.\n",
    "        Each of this pair should be a list which contains values of \n",
    "        the respective quantity at each iteration. Look at the usage of this\n",
    "        function in following blocks for better understanding.\n",
    "        \n",
    "        method - A string which denotes the method used for optimization.\n",
    "        It is only used in the title of the plots.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of iterations.\n",
    "    # Subtracting 1, since it also contains starting point\n",
    "    num_itr = len(history[\"x1\"]) - 1\n",
    "    \n",
    "    # Defining x and y values\n",
    "    x = np.linspace(-2,4,100)\n",
    "    y = np.linspace(-2,4,100)\n",
    "\n",
    "    # Creating a mesh at which values \n",
    "    # will be evaluated and plotted\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    # Evaluating the function values at meshpoints\n",
    "    x1 = X.reshape(-1,)\n",
    "    x2 = Y.reshape(-1,)\n",
    "    Z = np.zeros_like(x1)\n",
    "\n",
    "    # Calculating function value at each grid point\n",
    "    for i in range(len(x1)):\n",
    "            Z[i] = jones_function(np.array([x1[i],x2[i]]))\n",
    "\n",
    "    Z = Z.reshape(X.shape)\n",
    "\n",
    "    # Plotting the convergence history\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    ax.plot(np.arange(num_itr+1), history[\"x1\"], \"k\", marker=\".\", label=\"$x_1$\")\n",
    "    ax.plot(np.arange(num_itr+1), history[\"x2\"], \"b\", marker=\".\", label=\"$x_2$\")\n",
    "    ax.plot(np.arange(num_itr+1), history[\"f\"], \"g\", marker=\".\", label=\"$f$\")\n",
    "    ax.set_xlabel(\"Iterations\", fontsize=14)\n",
    "    ax.set_xlim(left=0, right=num_itr)\n",
    "    ax.set_ylabel(\"Quantities\", fontsize=14)\n",
    "    ax.grid()\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.set_title(\"Convergence history - \" + method, fontsize=14)\n",
    "    \n",
    "    # Denoting at which level to add contour lines\n",
    "    levels = np.arange(-13,-5,1)\n",
    "    levels = np.concatenate((levels, np.arange(-4, 8, 3)))\n",
    "    levels = np.concatenate((levels, np.arange(10, 100, 15)))\n",
    "\n",
    "    # Plotting the contours\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    CS = ax.contour(X, Y, Z, levels=levels, colors=\"k\", linestyles=\"solid\")\n",
    "    ax.clabel(CS, inline=1, fontsize=8)\n",
    "    ax.plot(history[\"x1\"], history[\"x2\"], \"k--\", marker=\".\", label=\"Path\", zorder=100.0)\n",
    "    ax.scatter(x0[0], x0[1], label=\"Starting point\", c=\"red\", zorder=10.0)\n",
    "    ax.scatter(result.x[0], result.x[1], label=\"Final point\", c=\"magenta\", zorder=10.0)\n",
    "    ax.set_xlabel(\"$x_1$\", fontsize=14)\n",
    "    ax.set_ylabel(\"$x_2$\", fontsize=14)\n",
    "    ax.set_title(\"Path of optimizer - starting point: {}\".format(starting_point), fontsize=14)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d9063",
   "metadata": {},
   "source": [
    "Below block of code performs gradient based optimization with different starting points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa6b82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Solver\n",
    "method = \"BFGS\"\n",
    "\n",
    "# Defines which finite difference scheme to use. Possible values are:\n",
    "# \"2-point\" - forward/backward difference\n",
    "# \"3-point\" - central difference\n",
    "# \"cs\" - complex step\n",
    "jac = \"2-point\"\n",
    "\n",
    "# Solver options\n",
    "options ={\n",
    "    \"disp\": True\n",
    "}\n",
    "\n",
    "################################ Starting point 1\n",
    "x0 = np.array([1.5, 3.5])\n",
    "\n",
    "# Defining dict for storing history of optimization\n",
    "history = {}\n",
    "history[\"x1\"] = [x0[0]]\n",
    "history[\"x2\"] = [x0[1]]\n",
    "history[\"f\"] = [jones_function(x0)]\n",
    "\n",
    "# Minimize the function\n",
    "result = minimize(fun=jones_function, x0=x0, method=method, jac=jac, callback=jones_opt_history, options=options)\n",
    "\n",
    "# Print value of x\n",
    "print(\"Value of x1 at optimum: {}\".format(result.x[0]))\n",
    "print(\"Value of x2 at optimum: {}\".format(result.x[1]))\n",
    "\n",
    "# Convergence plots\n",
    "jones_func_opt_plots(history, starting_point=x0)\n",
    "\n",
    "################################ Starting point 2\n",
    "x0 = np.array([-1, 0])\n",
    "\n",
    "# Defining dict for storing history of optimization\n",
    "history = {}\n",
    "history[\"x1\"] = [x0[0]]\n",
    "history[\"x2\"] = [x0[1]]\n",
    "history[\"f\"] = [jones_function(x0)]\n",
    "\n",
    "# Minimize the function\n",
    "result = minimize(fun=jones_function, x0=x0, method=method, jac=jac, callback=jones_opt_history, options=options)\n",
    "\n",
    "# Print value of x\n",
    "print(\"Value of x1 at optimum: {}\".format(result.x[0]))\n",
    "print(\"Value of x2 at optimum: {}\".format(result.x[1]))\n",
    "\n",
    "# Convergence plots\n",
    "jones_func_opt_plots(history, starting_point=x0)\n",
    "\n",
    "################################ Starting point 3\n",
    "x0 = np.array([-1, -1])\n",
    "\n",
    "# Defining dict for storing history of optimization\n",
    "history = {}\n",
    "history[\"x1\"] = [x0[0]]\n",
    "history[\"x2\"] = [x0[1]]\n",
    "history[\"f\"] = [jones_function(x0)]\n",
    "\n",
    "# Minimize the function\n",
    "result = minimize(fun=jones_function, x0=x0, method=method, jac=jac, callback=jones_opt_history, options=options)\n",
    "\n",
    "# Print value of x\n",
    "print(\"Value of x1 at optimum: {}\".format(result.x[0]))\n",
    "print(\"Value of x2 at optimum: {}\".format(result.x[1]))\n",
    "\n",
    "# Convergence plots\n",
    "jones_func_opt_plots(history, starting_point=x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22648cc",
   "metadata": {},
   "source": [
    "You can notice that with different starting points, the optimum obtained is different. This confirms the multi-modal nature of design space and the need for global optimization algorithm.\n",
    "\n",
    "### Multi-start gradient based optimization\n",
    "\n",
    "As described in the lecture, it involves gradient-based optimization with different starting points and finally selecting the best (either maximum or minmum depending on the problem) value. Below block of code performs gradient based optimization with 9 different starting points and prints the results. Read comments in the code for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253787e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solver\n",
    "method = \"BFGS\"\n",
    "\n",
    "# Defines which finite difference scheme to use. Possible values are:\n",
    "# \"2-point\" - forward/backward difference\n",
    "# \"3-point\" - central difference\n",
    "# \"cs\" - complex step\n",
    "jac = \"2-point\"\n",
    "\n",
    "# Solver options\n",
    "options ={\n",
    "    \"disp\": False\n",
    "}\n",
    "\n",
    "# Creating starting points\n",
    "num_pts = 3 # number of points in each direction\n",
    "x = np.linspace(-1.5, 3.5, num_pts)\n",
    "y = np.linspace(-1.5, 3.5, num_pts)\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "# Reshaping the array into 1D array\n",
    "# Total points be square of num_pts\n",
    "X = X.reshape(-1,)\n",
    "Y = Y.reshape(-1,)\n",
    "\n",
    "# Performing multi-start optimization\n",
    "for index in range(len(X)):\n",
    "    starting_point = np.array([X[index], Y[index]])\n",
    "    \n",
    "    # Minimize the function\n",
    "    result = minimize(fun=jones_function, x0=starting_point, method=method, jac=jac, options=options)\n",
    "\n",
    "    # Checking if the optimum found is better than current best point\n",
    "    if index == 0:\n",
    "        # Storing the best point\n",
    "        best_point = result.x\n",
    "        best_obj = jones_function(result.x)\n",
    "    else:\n",
    "        if jones_function(result.x) < best_obj:\n",
    "            best_point = result.x\n",
    "            best_obj = jones_function(result.x)\n",
    "            \n",
    "    print(\"Iteration {}:\".format(index+1))\n",
    "    print(\"Starting point: {}, Optimum found at: {}\".format(result.x, jones_function(result.x)))\n",
    "    \n",
    "print(\"\\nResult:\")\n",
    "print(\"{} is the best value found at x1 = {} and x2 = {}.\".format(best_obj, best_point[0], best_point[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c534a7ea",
   "metadata": {},
   "source": [
    "### Differential evolution\n",
    "\n",
    "[Differential evolution](https://en.wikipedia.org/wiki/Differential_evolution) is a global optimization algorithm available in `scipy.optimize` module. There are various parameters for this method, please read the [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html) for more details.\n",
    "\n",
    "Below block of code defines couple of functions which are used during optimization. Read comments in the code for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dbd206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jones_global_opt_history(x, convergence):\n",
    "    \"\"\"\n",
    "        Function which is called after every iteration of optimization.\n",
    "        It stores the value of x1, x2, function value, and convergence which is\n",
    "        later used for plotting convergence history.\n",
    "        \n",
    "        Note: This function is slightly different from the one used for gradient\n",
    "        based optimization. There is one more variable which is given to optimizer.\n",
    "        \n",
    "        Input: \n",
    "        x - 1d numpy array which contains current x value\n",
    "        convergence - represents the fractional value of the population convergence\n",
    "        Output: None\n",
    "    \"\"\"\n",
    "    \n",
    "    history[\"x1\"].append(x[0])\n",
    "    history[\"x2\"].append(x[1])\n",
    "    history[\"f\"].append(jones_function(x))\n",
    "    history[\"convergence\"].append(convergence)\n",
    "    \n",
    "def jones_func_global_opt_plots(history):\n",
    "    \"\"\"\n",
    "        Function used for plotting the results of the optimization.\n",
    "        \n",
    "        Input: \n",
    "        history - A dictionary which contains three key-value pairs - x1, x2, and f.\n",
    "        Each of this pair should be a list which contains values of \n",
    "        the respective quantity at each iteration. Look at the usage of this\n",
    "        function in following blocks for better understanding.\n",
    "        \n",
    "        method - A string which denotes the method used for optimization.\n",
    "        It is only used in the title of the plots.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of iterations\n",
    "    num_itr = len(history[\"x1\"])\n",
    "    \n",
    "    # Defining x and y values\n",
    "    x = np.linspace(-4,4,50)\n",
    "    y = np.linspace(-4,4,50)\n",
    "    \n",
    "    # Creating a mesh at which values \n",
    "    # will be evaluated and plotted\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Evaluating the function values at meshpoints\n",
    "    x1 = X.reshape(-1,)\n",
    "    x2 = Y.reshape(-1,)\n",
    "    Z = np.zeros_like(x1)\n",
    "\n",
    "    # Calculating function value at each grid point\n",
    "    for i in range(len(x1)):\n",
    "            Z[i] = jones_function(np.array([x1[i],x2[i]]))\n",
    "\n",
    "    Z = Z.reshape(X.shape)\n",
    "    \n",
    "    # Plotting the convergence history\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(13,5))\n",
    "    \n",
    "    ax[0].plot(np.arange(num_itr)+1, history[\"x1\"], \"k\", marker=\".\", label=\"$x_1$\")\n",
    "    ax[0].plot(np.arange(num_itr)+1, history[\"x2\"], \"b\", marker=\".\", label=\"$x_2$\")\n",
    "    ax[0].plot(np.arange(num_itr)+1, history[\"f\"], \"g\", marker=\".\", label=\"$f$\")\n",
    "    ax[0].set_xlabel(\"Iterations\", fontsize=14)\n",
    "    ax[0].set_xlim(left=1, right=num_itr)\n",
    "    ax[0].set_ylabel(\"Quantities\", fontsize=14)\n",
    "    ax[0].grid()\n",
    "    ax[0].legend(fontsize=12)\n",
    "    \n",
    "    ax[1].plot(np.arange(num_itr)+1, history[\"convergence\"], \"g\", marker=\".\", label=\"$f$\")\n",
    "    ax[1].set_xlabel(\"Iterations\", fontsize=14)\n",
    "    ax[1].set_xlim(left=1, right=num_itr)\n",
    "    ax[1].set_ylabel(\"Population convergence\", fontsize=14)\n",
    "    ax[1].grid()\n",
    "    \n",
    "    fig.suptitle(\"Convergence history\", fontsize=14)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25708ba7",
   "metadata": {},
   "source": [
    "Below block of code optimizes jones function using differential evolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea863e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Defining dict for storing history of optimization\n",
    "history = {}\n",
    "history[\"x1\"] = []\n",
    "history[\"x2\"] = []\n",
    "history[\"f\"] = []\n",
    "history[\"convergence\"] = []\n",
    "\n",
    "# Bounds\n",
    "bounds = [(-2,4), (-2,4)]\n",
    "\n",
    "# There are various paramters which can be set for differential evolution.\n",
    "# Please check the documentation\n",
    "result = optimize.differential_evolution(jones_function, disp=True, bounds=bounds, init=\"latinhypercube\", callback=jones_global_opt_history)\n",
    "\n",
    "# Print value of x\n",
    "print(\"Value of x1 at optimum: {}\".format(result.x[0]))\n",
    "print(\"Value of x2 at optimum: {}\".format(result.x[1]))\n",
    "print(\"Function value at optimum: {}\".format(jones_function(result.x)))\n",
    "\n",
    "# Convergence plots\n",
    "jones_func_global_opt_plots(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8412f3a",
   "metadata": {},
   "source": [
    "*First* plot shows the best solution in each iteration. *Second* plot shows the fractional value of population convergence. As soon as this value goes above 1.0, optimization is terminated. Also, when you run above block of code once again, you will notice that convergence history changes slightly which shows that there is some randomness in the algorithm. Most global optimization algorithm have some kind of randomness in their method, so it is always advised to run the optimization few times to ensure that you get more or less the same answer everytime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
